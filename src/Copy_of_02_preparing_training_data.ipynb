{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g2WKuIv5tPg"
   },
   "source": [
    "# Preparing training data\n",
    "\n",
    "In this tutorial, we will convert medical imaging data to the TFRecords format. Having data in the TFRecords format speeds up training and allows us to use standard, highly-optimized TensorFlow I/O methods. We will then create a `tf.data.Dataset` object using the TFRecords data. This `tf.data.Dataset` object can be used for training, evaluation, or prediction.\n",
    "\n",
    "This tutorial will use publicly available data. To convert your own data, you will need to create a nested list of features and labels volumes. One can store this as a CSV that looks like the following:\n",
    "\n",
    "```\n",
    "features,labels\n",
    "/path/to/1_features.nii.gz,/path/to/1_labels.nii.gz\n",
    "/path/to/2_features.nii.gz,/path/to/2_labels.nii.gz\n",
    "/path/to/3_features.nii.gz,/path/to/3_labels.nii.gz\n",
    "/path/to/4_features.nii.gz,/path/to/4_labels.nii.gz\n",
    "```\n",
    "\n",
    "\n",
    "You can read this CSV in Python with `nobrainer.io.read_csv`.\n",
    "\n",
    "## Google Colaboratory\n",
    "\n",
    "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU is not necessary to prepare the data, but a GPU is helpful for training a model, which we demonstrate at the end of this notebook.\n",
    "\n",
    "**Note:** To complete the last part of this tutorial, which involves training, a GPU is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knowOCwX5ukM",
    "outputId": "d7b79c74-ce41-4bd9-c785-c8603e345bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nobrainer\n",
      "  Downloading nobrainer-0.0.3-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\esink\\anaconda3\\lib\\site-packages (from nobrainer) (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\esink\\anaconda3\\lib\\site-packages (from nobrainer) (1.19.5)\n",
      "Requirement already satisfied: click in c:\\users\\esink\\anaconda3\\lib\\site-packages (from nobrainer) (7.1.2)\n",
      "Collecting tensorflow-probability>=0.7.0\n",
      "  Downloading tensorflow_probability-0.12.2-py2.py3-none-any.whl (4.8 MB)\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-3.2.1-py3-none-any.whl (3.3 MB)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (1.6.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (3.3.4)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (2.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (8.0.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from scikit-image->nobrainer) (1.1.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7.0->nobrainer) (1.15.0)\n",
      "Collecting gast>=0.3.2\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-win_amd64.whl (75 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7.0->nobrainer) (1.6.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7.0->nobrainer) (4.4.2)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from nibabel->nobrainer) (20.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (2.4.7)\n",
      "Installing collected packages: gast, dm-tree, tensorflow-probability, nibabel, nobrainer\n",
      "Successfully installed dm-tree-0.1.6 gast-0.4.0 nibabel-3.2.1 nobrainer-0.0.3 tensorflow-probability-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-win_amd64.whl (422.6 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-py2.py3-none-any.whl (173 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\esink\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33698 sha256=7d74c71cec15aee6a93fe0f5bcbd7376d4f46120181465651195e0a1f96273a9\n",
      "  Stored in directory: C:\\Users\\esink\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-na0ywmk6\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=90c274e8bc69ee0ed4d4e4fbb13d3864f2d0c770412b37c35aaefb281e866460\n",
      "  Stored in directory: C:\\Users\\esink\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-na0ywmk6\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: astunparse, flatbuffers, wrapt, protobuf, tensorflow-estimator, google-pasta, h5py, grpcio, termcolor, keras-nightly, keras-preprocessing, absl-py, tensorboard-data-server, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, opt-einsum, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 google-auth-1.31.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "egda7m595tPi"
   },
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeWmDZXq5tPj"
   },
   "source": [
    "## Get sample data\n",
    "\n",
    "Here, we download 10 T1-weighted brain scans and their corresponding FreeSurfer segmentations. These volumes take up about 46 MB and are saved to a temporary directory. The object `csv_path` is the path to a CSV file. Each row of this CSV file contains the paths to a pair of features and labels volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1DD5tCh5tPk",
    "outputId": "ab00e73f-e698-4475-ce8a-a95fe9482dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-01/mri/t1.mgz\n",
      "4276224/4268641 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-01/mri/aparc+aseg.mgz\n",
      "344064/339508 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-02/mri/t1.mgz\n",
      "3366912/3360239 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-02/mri/aparc+aseg.mgz\n",
      "360448/354161 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-03/mri/t1.mgz\n",
      "3907584/3900552 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-03/mri/aparc+aseg.mgz\n",
      "352256/350451 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-04/mri/t1.mgz\n",
      "5120000/5115676 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-04/mri/aparc+aseg.mgz\n",
      "425984/419833 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-05/mri/t1.mgz\n",
      "4743168/4738958 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-05/mri/aparc+aseg.mgz\n",
      "417792/417547 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-06/mri/t1.mgz\n",
      "4964352/4956283 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-06/mri/aparc+aseg.mgz\n",
      "409600/403496 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-07/mri/t1.mgz\n",
      "4276224/4271015 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-07/mri/aparc+aseg.mgz\n",
      "360448/356833 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-08/mri/t1.mgz\n",
      "4702208/4702059 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-08/mri/aparc+aseg.mgz\n",
      "417792/414228 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-09/mri/t1.mgz\n",
      "4227072/4220278 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-09/mri/aparc+aseg.mgz\n",
      "401408/395977 [==============================] - 0s 1us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-10/mri/t1.mgz\n",
      "3874816/3868870 [==============================] - 1s 0us/step\n",
      "Downloading data from https://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/freesurfer/sub-10/mri/aparc+aseg.mgz\n",
      "335872/335557 [==============================] - 0s 1us/step\n",
      "features,labels\n",
      "/tmp/nobrainer-data/datasets/sub-01_t1.mgz,/tmp/nobrainer-data/datasets/sub-01_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-02_t1.mgz,/tmp/nobrainer-data/datasets/sub-02_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-03_t1.mgz,/tmp/nobrainer-data/datasets/sub-03_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-04_t1.mgz,/tmp/nobrainer-data/datasets/sub-04_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-05_t1.mgz,/tmp/nobrainer-data/datasets/sub-05_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-06_t1.mgz,/tmp/nobrainer-data/datasets/sub-06_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-07_t1.mgz,/tmp/nobrainer-data/datasets/sub-07_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-08_t1.mgz,/tmp/nobrainer-data/datasets/sub-08_aparc+aseg.mgz\n",
      "/tmp/nobrainer-data/datasets/sub-09_t1.mgz,/tmp/nobrainer-data/datasets/sub-09_aparc+aseg.mgz\n"
     ]
    }
   ],
   "source": [
    "csv_path = nobrainer.utils.get_data()\n",
    "\n",
    "!head $csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Xtztk4nD-WdG",
    "outputId": "f0def339-fe2b-4a12-feb2-1c2b1fd28b04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/tmp/nobrainer-data/filepaths.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm8aVxsc5tPk"
   },
   "source": [
    "## Convert to volume files to TFRecords\n",
    "\n",
    "To achieve the best performance, training data should be in TFRecords format. This is the preferred file format for TensorFlow, Training can be done on medical imaging volume files but will be slower.\n",
    "\n",
    "Nobrainer has a command-line utility to convert volumes to TFRecords: `nobrainer convert`. This will verify that all of the volumes have the same shape and that the label volumes are an integer type or can be safely coerced to an integer type. \n",
    "\n",
    "Following successful verification, the volumes will be converted to TFRecords files. The dataset should be sharded into multiple TFRecords files, so that data can be shuffled more properly. This is especially helpful for large datasets. Users can choose how many pairs of volumes (i.e., features and labels) will be saved to one TFRecords file. In this example, we will save 3 pairs of volumes per TFRecords file because our dataset is small. With a larger dataset, users should choose a larger shard value. For example, with 10,000 volumes, one might choose 100 volumes per TFRecords file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ehrrXhs5tPk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!nobrainer convert --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSb9uqaG5tPl"
   },
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFRDPcKQ5tPl"
   },
   "outputs": [],
   "source": [
    "!nobrainer convert \\\n",
    "    --csv='/tmp/nobrainer-data/filepaths.csv' \\\n",
    "    --tfrecords-template='data/data_shard-{shard:03d}.tfrec' \\\n",
    "    --examples-per-shard=3 \\\n",
    "    --volume-shape 256 256 256 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oG79IlJ5tPl"
   },
   "source": [
    "# Create input data pipeline\n",
    "\n",
    "We will now create an data pipeline to feed our models with training data. The steps below will create a `tensorflow.data.Dataset` object that is built according to [TensorFlow's guidelines](https://www.tensorflow.org/guide/performance/datasets). The basic pipeline is summarized below.\n",
    "\n",
    "- Read data\n",
    "- Separate volumes into non-overlapping sub-volumes\n",
    "    - This is done to get around memory limitations with larger models.\n",
    "    - For example, a volume with shape (256, 256, 256) can be separated into eight non-overlapping blocks of shape (128, 128, 128).\n",
    "- Apply random rigid augmentations if requested.\n",
    "- Standard score volumes of features.\n",
    "- Binarize labels if binary segmentation.\n",
    "- Replace values according to some mapping if multi-class segmentation.\n",
    "- Batch the results so every iteration yields `batch_size` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au6YBWec5tPm"
   },
   "outputs": [],
   "source": [
    "# A glob pattern to match the files we want to train on.\n",
    "file_pattern = 'data/data_shard-*.tfrec'\n",
    "\n",
    "# The number of classes the model predicts. A value of 1 means the model performs\n",
    "# binary classification (i.e., target vs background).\n",
    "n_classes = 1\n",
    "\n",
    "# Batch size is the number of features and labels we train on with each step.\n",
    "batch_size = 2\n",
    "\n",
    "# The shape of the original volumes.\n",
    "volume_shape = (256, 256, 256)\n",
    "\n",
    "# The shape of the non-overlapping sub-volumes. Most models cannot be trained on\n",
    "# full volumes because of hardware and memory constraints, so we train and evaluate\n",
    "# on sub-volumes.\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "# Whether or not to apply random rigid transformations to the data on the fly.\n",
    "# This can improve model generalizability but increases processing time.\n",
    "augment = False\n",
    "\n",
    "# The tfrecords filepaths will be shuffled before reading, but we can also shuffle\n",
    "# the data. This will shuffle 10 volumes at a time. Larger buffer sizes will require\n",
    "# more memory, so choose a value based on how much memory you have available.\n",
    "shuffle_buffer_size = 10\n",
    "\n",
    "# Number of parallel processes to use.\n",
    "num_parallel_calls = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOLBC_8R5tPn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $file_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qaZ8eoe5tPn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = nobrainer.dataset.get_dataset(\n",
    "    file_pattern=file_pattern,\n",
    "    n_classes=n_classes,\n",
    "    batch_size=batch_size,\n",
    "    volume_shape=volume_shape,\n",
    "    block_shape=block_shape,\n",
    "    augment=augment,\n",
    "    n_epochs=1,\n",
    "    shuffle_buffer_size=shuffle_buffer_size,\n",
    "    num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM8fATVF5tPn"
   },
   "source": [
    "# Train a model\n",
    "\n",
    "We will briefly demonstrate how to train a model given the `tf.data.Dataset` we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYrNoHSx5tPo"
   },
   "source": [
    "## Instantiate a pre-defined `nobrainer` model\n",
    "\n",
    "Users can find pre-defined models under the namespace `nobrainer.models`. All models are implemented using the `tf.keras` API, which makes definitions highly readable and hackable, despite being a high-level interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPDE6n8e5tPo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nobrainer.models.unet(n_classes=n_classes, input_shape=(*block_shape, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEo-bJAo5tPo"
   },
   "source": [
    "## Compile the model\n",
    "\n",
    "All Keras models must be compiled before they can be trained. This is where you choose the optimizer, loss function, and any metrics that should be reported during training. Nobrainer implements several loss functions useful for semantic segmentation, including Dice, Generalized Dice, Focal, Jaccard, and Tversky losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvhD-oAf5tPo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfB-X1RP5tPp"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-04),\n",
    "    loss=nobrainer.losses.jaccard,\n",
    "    metrics=[nobrainer.metrics.dice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhXfMXI_5tPp"
   },
   "source": [
    "## Train on a single GPU\n",
    "\n",
    "To learn how to train on multiple GPUs or on a TPU, please refer to the other notebooks in the nobrainer guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGhxkll75tPp"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = nobrainer.dataset.get_steps_per_epoch(\n",
    "    n_volumes=10, \n",
    "    volume_shape=volume_shape, \n",
    "    block_shape=block_shape, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rIE6pL7b2O"
   },
   "source": [
    "The following code may take some time to initialize and on a GPU (if enabled) will take about a minute and a half to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIYcof_M5tPp"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, steps_per_epoch=steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of 02-preparing_training_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
